{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMC8UilTf9D4"
      },
      "source": [
        "# Setting Up PyTorch and CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSuDRpKaf9D5",
        "outputId": "e888823b-d51e-4756-fc47-54ddb50e2471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCd0-wSgf9D6",
        "outputId": "6aafa821-fdea-4c34-fca0-88354ae52031"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: transformers[torch] in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (4.41.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (0.23.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: torch in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers[torch]) (0.31.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch->transformers[torch]) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->transformers[torch]) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: accelerate in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: requests in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (3.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\pete\\onedrive\\desktop\\404_finalproj\\torchenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas nltk transformers[torch]\n",
        "%pip install accelerate -U\n",
        "%pip install transformers\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnBEylpyf9D6",
        "outputId": "113c4b5b-f1f7-45ea-ffdc-17e142bd69a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pete\\OneDrive\\Desktop\\404_finalproj\\torchenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#Imports here, no need for structure\n",
        "import os, sys, json, re, string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBsFlCtIf9D6"
      },
      "outputs": [],
      "source": [
        "!set PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\bin;%PATH%\n",
        "!set PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\lib\\x64;%PATH%\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY604bJaf9D6",
        "outputId": "751efe1c-8dd1-4d83-a362-9059fae382ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated PATH to include: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\bin\n"
          ]
        }
      ],
      "source": [
        "cuda_bin_path = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\bin\"\n",
        "if cuda_bin_path not in os.environ[\"PATH\"]:\n",
        "    os.environ[\"PATH\"] = cuda_bin_path + \";\" + os.environ[\"PATH\"]\n",
        "print(\"Updated PATH to include:\", cuda_bin_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx19-bgNf9D7"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "SpzgoIPu5wIV",
        "outputId": "884e69c1-2159-43f3-e23c-17beed922bed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>score</th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>comms_num</th>\n",
              "      <th>created</th>\n",
              "      <th>body</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE</td>\n",
              "      <td>317</td>\n",
              "      <td>l6uf6d</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>53</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>Hedgefund whales are spreading disinfo saying ...</td>\n",
              "      <td>2021-01-28 21:26:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>THIS IS THE MOMENT</td>\n",
              "      <td>405</td>\n",
              "      <td>l6ub9l</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>178</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>Life isn't fair. My mother always told me that...</td>\n",
              "      <td>2021-01-28 21:19:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>We need to keep this movement going, we all ca...</td>\n",
              "      <td>222</td>\n",
              "      <td>l6uao1</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>70</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>I believe right now is one of those rare oppo...</td>\n",
              "      <td>2021-01-28 21:18:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>$GME price has nothing to do with fundamentals...</td>\n",
              "      <td>382</td>\n",
              "      <td>l6u96y</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>61</td>\n",
              "      <td>1.611861e+09</td>\n",
              "      <td>Firstly, all of you diamond hands and smooth-b...</td>\n",
              "      <td>2021-01-28 21:15:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Daily Discussion Thread for January 28, 2021</td>\n",
              "      <td>841</td>\n",
              "      <td>l6u011</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>5942</td>\n",
              "      <td>1.611860e+09</td>\n",
              "      <td>Your daily trading discussion thread. Please k...</td>\n",
              "      <td>2021-01-28 21:00:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53177</th>\n",
              "      <td>$CRSR August 20, 2021 Unusual Options Activity üëÄ</td>\n",
              "      <td>200</td>\n",
              "      <td>owhamt</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>115</td>\n",
              "      <td>1.627919e+09</td>\n",
              "      <td>A)50$ strike\\n3,048 Open Interest\\n310 Volume ...</td>\n",
              "      <td>2021-08-02 18:49:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53181</th>\n",
              "      <td>Ten Year Price Prediction for TSLA</td>\n",
              "      <td>156</td>\n",
              "      <td>owfbxp</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>204</td>\n",
              "      <td>1.627913e+09</td>\n",
              "      <td>It‚Äôs all contingent on them mastering FSD, but...</td>\n",
              "      <td>2021-08-02 17:11:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53182</th>\n",
              "      <td>What I Learned Investigating SAVA FUD Spreaders</td>\n",
              "      <td>238</td>\n",
              "      <td>owd2pn</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>87</td>\n",
              "      <td>1.627906e+09</td>\n",
              "      <td>***TLDR: Three bitter scientists partnered up ...</td>\n",
              "      <td>2021-08-02 15:03:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53183</th>\n",
              "      <td>Daily Popular Tickers Thread for August 02, 20...</td>\n",
              "      <td>228</td>\n",
              "      <td>owd1a5</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>1070</td>\n",
              "      <td>1.627906e+09</td>\n",
              "      <td>\\nYour daily hype thread. Please keep the shit...</td>\n",
              "      <td>2021-08-02 15:01:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53185</th>\n",
              "      <td>Daily Discussion Thread for August 02, 2021</td>\n",
              "      <td>338</td>\n",
              "      <td>owbfjf</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>11688</td>\n",
              "      <td>1.627898e+09</td>\n",
              "      <td>Your daily trading discussion thread. Please k...</td>\n",
              "      <td>2021-08-02 13:00:16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6409 rows √ó 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  score      id  \\\n",
              "6            SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE    317  l6uf6d   \n",
              "7                                     THIS IS THE MOMENT    405  l6ub9l   \n",
              "10     We need to keep this movement going, we all ca...    222  l6uao1   \n",
              "13     $GME price has nothing to do with fundamentals...    382  l6u96y   \n",
              "19          Daily Discussion Thread for January 28, 2021    841  l6u011   \n",
              "...                                                  ...    ...     ...   \n",
              "53177   $CRSR August 20, 2021 Unusual Options Activity üëÄ    200  owhamt   \n",
              "53181                 Ten Year Price Prediction for TSLA    156  owfbxp   \n",
              "53182    What I Learned Investigating SAVA FUD Spreaders    238  owd2pn   \n",
              "53183  Daily Popular Tickers Thread for August 02, 20...    228  owd1a5   \n",
              "53185        Daily Discussion Thread for August 02, 2021    338  owbfjf   \n",
              "\n",
              "                                                     url  comms_num  \\\n",
              "6      https://www.reddit.com/r/wallstreetbets/commen...         53   \n",
              "7      https://www.reddit.com/r/wallstreetbets/commen...        178   \n",
              "10     https://www.reddit.com/r/wallstreetbets/commen...         70   \n",
              "13     https://www.reddit.com/r/wallstreetbets/commen...         61   \n",
              "19     https://www.reddit.com/r/wallstreetbets/commen...       5942   \n",
              "...                                                  ...        ...   \n",
              "53177  https://www.reddit.com/r/wallstreetbets/commen...        115   \n",
              "53181  https://www.reddit.com/r/wallstreetbets/commen...        204   \n",
              "53182  https://www.reddit.com/r/wallstreetbets/commen...         87   \n",
              "53183  https://www.reddit.com/r/wallstreetbets/commen...       1070   \n",
              "53185  https://www.reddit.com/r/wallstreetbets/commen...      11688   \n",
              "\n",
              "            created                                               body  \\\n",
              "6      1.611862e+09  Hedgefund whales are spreading disinfo saying ...   \n",
              "7      1.611862e+09  Life isn't fair. My mother always told me that...   \n",
              "10     1.611862e+09   I believe right now is one of those rare oppo...   \n",
              "13     1.611861e+09  Firstly, all of you diamond hands and smooth-b...   \n",
              "19     1.611860e+09  Your daily trading discussion thread. Please k...   \n",
              "...             ...                                                ...   \n",
              "53177  1.627919e+09  A)50$ strike\\n3,048 Open Interest\\n310 Volume ...   \n",
              "53181  1.627913e+09  It‚Äôs all contingent on them mastering FSD, but...   \n",
              "53182  1.627906e+09  ***TLDR: Three bitter scientists partnered up ...   \n",
              "53183  1.627906e+09  \\nYour daily hype thread. Please keep the shit...   \n",
              "53185  1.627898e+09  Your daily trading discussion thread. Please k...   \n",
              "\n",
              "                 timestamp  \n",
              "6      2021-01-28 21:26:27  \n",
              "7      2021-01-28 21:19:31  \n",
              "10     2021-01-28 21:18:25  \n",
              "13     2021-01-28 21:15:58  \n",
              "19     2021-01-28 21:00:15  \n",
              "...                    ...  \n",
              "53177  2021-08-02 18:49:40  \n",
              "53181  2021-08-02 17:11:36  \n",
              "53182  2021-08-02 15:03:27  \n",
              "53183  2021-08-02 15:01:03  \n",
              "53185  2021-08-02 13:00:16  \n",
              "\n",
              "[6409 rows x 8 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_path = r\"C:\\Users\\pete\\OneDrive\\Documents\\School\\WWU\\Year 3\\3. Spring\\CSCI 404\\final\\Reddit_Post_Generator\\reddit_wsb.csv\"\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "df.dropna(inplace=True)\n",
        "df = df[df['score'] >= 100]\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.head()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvoJooAN5wIW"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAV-YEKNf9D7"
      },
      "source": [
        "#### Verifying CUDA Works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grYpBmYzf9D7",
        "outputId": "b724ea5d-34aa-4b50-e359-95ef825f78bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiled with CUDA: 12.1\n"
          ]
        }
      ],
      "source": [
        "print(\"Compiled with CUDA:\", torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9lFKJ0hf9D7",
        "outputId": "b09a5f0b-ac7e-4af9-ebc6-f6d3fdc5e45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "0\n",
            "<torch.cuda.device object at 0x00000170135DBEE0>\n",
            "1\n",
            "NVIDIA GeForce RTX 3080\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UAqWUmrf9D8"
      },
      "source": [
        "# Building Model Based on Post Titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D31eVZT6f9D8"
      },
      "outputs": [],
      "source": [
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "train_dataset = [tokenizer.encode(title, add_special_tokens=True) for title in df['title']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CXSY8qc05wIW",
        "outputId": "cbfd9739-4757-40aa-b0e4-bb8375f51d5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|‚ñà         | 501/4809 [00:58<08:28,  8.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 6.8604, 'grad_norm': 11.436975479125977, 'learning_rate': 4.488459139114161e-05, 'epoch': 0.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|‚ñà‚ñà        | 1001/4809 [01:56<07:25,  8.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.4149, 'grad_norm': 8.69334602355957, 'learning_rate': 3.9686005406529424e-05, 'epoch': 0.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|‚ñà‚ñà‚ñà       | 1501/4809 [02:55<06:19,  8.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2269, 'grad_norm': 8.184319496154785, 'learning_rate': 3.448741942191724e-05, 'epoch': 0.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 2001/4809 [03:53<05:28,  8.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.7591, 'grad_norm': 9.584005355834961, 'learning_rate': 2.9288833437305052e-05, 'epoch': 1.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2501/4809 [04:51<04:30,  8.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.6223, 'grad_norm': 7.279341220855713, 'learning_rate': 2.409024745269287e-05, 'epoch': 1.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3001/4809 [05:50<03:33,  8.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.6346, 'grad_norm': 10.476033210754395, 'learning_rate': 1.8891661468080684e-05, 'epoch': 1.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3501/4809 [06:48<02:35,  8.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.3499, 'grad_norm': 7.503251552581787, 'learning_rate': 1.3693075483468498e-05, 'epoch': 2.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 4001/4809 [07:46<01:33,  8.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.2916, 'grad_norm': 7.031538009643555, 'learning_rate': 8.494489498856312e-06, 'epoch': 2.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4501/4809 [08:44<00:36,  8.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.2751, 'grad_norm': 8.937682151794434, 'learning_rate': 3.295903514244126e-06, 'epoch': 2.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4809/4809 [09:20<00:00,  8.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 560.5343, 'train_samples_per_second': 34.301, 'train_steps_per_second': 8.579, 'train_loss': 3.9974258495829758, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./fine-tuned-gpt2\\\\tokenizer_config.json',\n",
              " './fine-tuned-gpt2\\\\special_tokens_map.json',\n",
              " './fine-tuned-gpt2\\\\vocab.json',\n",
              " './fine-tuned-gpt2\\\\merges.txt',\n",
              " './fine-tuned-gpt2\\\\added_tokens.json')"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    logging_steps=500,\n",
        "    logging_dir='C:/Users/pete/OneDrive/Desktop/404_finalproj/logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained('./fine-tuned-gpt2')\n",
        "tokenizer.save_pretrained('./fine-tuned-gpt2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw7iVqRq5wIX",
        "outputId": "0d439583-07d5-438c-bab5-062cf447890d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pete\\OneDrive\\Desktop\\404_finalproj\\torchenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated post: Today in the stock market, we have the potential to change the entire market forever. We need to hold and hold. üíéüôåüí™üèºüöÄüåïüçå.üé•ü¶çüêªüñêÔ∏èü§≤üìâü°å üö®ü™ìüßªüëêü•ΩüõΩüëåüëäüëãüëÄüëçÔøΩüäÄ‚úã‚úä‚úå‚úç‚úé‚úè‚úê‚úî‚úñ‚úâ‚úí‚úö‚úà‚ú™‚ú•‚ú¶‚úù‚ú®‚ú´‚úÆ‚úØ‚ú≠‚ú°‚ú≤‚ú≥ÔøΩ\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = './fine-tuned-gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "def generate_post(prompt, max_length=150):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    generated_post = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_post\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Today in the stock market\"\n",
        "generated_post = generate_post(prompt)\n",
        "print(f\"Generated post: {generated_post}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lzk_67ff9D8"
      },
      "source": [
        "### Grabs random token from post titles, then generates a post based on it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvW7xMP8f9D8",
        "outputId": "e99e84a2-4a5c-44ce-bc3f-c28ae87c0c29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Token ID: 5984\n",
            "Random Token Text:  AD\n",
            "Generated post:  ADC - The Big Short is Coming - A Short Squeeze Analysis - Part 2 - DD Part 3 - Why I'm betting on $AMC and $BB to win the day. üöÄüöÇüíéüôåüèºüåïü¶çüçå üíåüëêüêªüéØü§≤ü™ìüìâü°å.üñêÔ∏èüî≠ü®Ç üîÆüßªüëåÔøΩ‚úå ‚úå‚òå ‚òåÔ∏è‚úä‚úã‚úç‚úé‚úè‚úê‚úî‚úâ‚úà‚úö‚úí‚úñÔ∏è‚òä ‚úä‚òé‚òê‚òâ\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Flatten the list of tokenized titles\n",
        "flat_token_list = [token for sublist in train_dataset for token in sublist]\n",
        "\n",
        "# Select a random token\n",
        "random_token = random.choice(flat_token_list)\n",
        "\n",
        "# Optional: Convert the token back to text to see what it is\n",
        "random_token_text = tokenizer.decode([random_token])\n",
        "\n",
        "print(\"Random Token ID:\", random_token)\n",
        "print(\"Random Token Text:\", random_token_text)\n",
        "\n",
        "generated_random_post = generate_post(random_token_text)\n",
        "print(f\"Generated post: {generated_random_post}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJdiriYBf9D8"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "def setup_summarization_model():\n",
        "    model_name = 'facebook/bart-large-cnn'  # Pre-trained model for summarization\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNf_FHxXf9D8"
      },
      "outputs": [],
      "source": [
        "def generate_title(post_body, tokenizer, model, max_length=50):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + post_body, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    post_title = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return post_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrixoopgf9D9"
      },
      "outputs": [],
      "source": [
        "# Setup the summarization model\n",
        "summarization_tokenizer, summarization_model = setup_summarization_model()\n",
        "\n",
        "# Generate a title for the post\n",
        "post_title = generate_title(generated_random_post, summarization_tokenizer, summarization_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to4rUjR3f9D9",
        "outputId": "03be547a-329f-456a-81fc-afd342fd0bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Token ID: 5984\n",
            "Random Token Text:  AD\n",
            "Generated title:  ADC - The Big Short is Coming - A Short Squeeze Analysis - Part 2 - DD Part 3 - Why I'm betting on $AMC and $BB to win the day.\n",
            "Generated post:  ADC - The Big Short is Coming - A Short Squeeze Analysis - Part 2 - DD Part 3 - Why I'm betting on $AMC and $BB to win the day. üöÄüöÇüíéüôåüèºüåïü¶çüçå üíåüëêüêªüéØü§≤ü™ìüìâü°å.üñêÔ∏èüî≠ü®Ç üîÆüßªüëåÔøΩ‚úå ‚úå‚òå ‚òåÔ∏è‚úä‚úã‚úç‚úé‚úè‚úê‚úî‚úâ‚úà‚úö‚úí‚úñÔ∏è‚òä ‚úä‚òé‚òê‚òâ\n"
          ]
        }
      ],
      "source": [
        "print(\"Random Token ID:\", random_token)\n",
        "print(\"Random Token Text:\", random_token_text)\n",
        "print(f\"Generated title: {post_title[11:]}\")\n",
        "print(f\"Generated post: {generated_random_post}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMzO9VQ4f9D9"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGK8IWbAf9D9"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "data_path = r\"C:\\Users\\pete\\OneDrive\\Documents\\School\\WWU\\Year 3\\3. Spring\\CSCI 404\\final\\Reddit_Post_Generator\\reddit_wsb.csv\"\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "df.dropna(inplace=True)\n",
        "df = df[df['score'] >= 100]\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.head()\n",
        "\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "df['word_count'] = df['body'].apply(count_words)\n",
        "df.drop('word_count', axis=1, inplace=True)\n",
        "df['normal_body'] = df['body'].apply(normalize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax-gfFsRf9D9"
      },
      "source": [
        "# Generating Post Based on Body Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfZQpeASf9D9"
      },
      "outputs": [],
      "source": [
        "max_length = 64  # Define the maximum sequence length for GPT-2\n",
        "\n",
        "# Encode the dataset with truncation and padding\n",
        "body_train_dataset = [\n",
        "    tokenizer.encode(\n",
        "        body,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        add_special_tokens=True\n",
        "    ) for body in df['body']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YpB8LMLf9D9"
      },
      "source": [
        "### Setting up the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCVg3IMLf9D9",
        "outputId": "fe614b2c-dba7-4e5e-a6c8-845ac5d23c48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|‚ñà         | 501/4809 [00:57<08:20,  8.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 5.9946, 'grad_norm': 5.952375888824463, 'learning_rate': 4.488459139114161e-05, 'epoch': 0.31}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|‚ñà‚ñà        | 1001/4809 [01:55<07:19,  8.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.5796, 'grad_norm': 5.326918125152588, 'learning_rate': 3.9686005406529424e-05, 'epoch': 0.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|‚ñà‚ñà‚ñà       | 1501/4809 [02:52<06:19,  8.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.4727, 'grad_norm': 4.926661968231201, 'learning_rate': 3.448741942191724e-05, 'epoch': 0.94}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 2001/4809 [03:49<05:16,  8.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1786, 'grad_norm': 4.754562854766846, 'learning_rate': 2.9288833437305052e-05, 'epoch': 1.25}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 2501/4809 [04:45<04:17,  8.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1676, 'grad_norm': 4.745387554168701, 'learning_rate': 2.409024745269287e-05, 'epoch': 1.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3001/4809 [05:42<03:24,  8.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1335, 'grad_norm': 4.456200122833252, 'learning_rate': 1.8891661468080684e-05, 'epoch': 1.87}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 3501/4809 [06:38<02:27,  8.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9982, 'grad_norm': 4.387679576873779, 'learning_rate': 1.3693075483468498e-05, 'epoch': 2.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 4001/4809 [07:34<01:31,  8.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9932, 'grad_norm': 5.719286918640137, 'learning_rate': 8.494489498856312e-06, 'epoch': 2.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 4501/4809 [08:31<00:34,  8.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9558, 'grad_norm': 4.396152973175049, 'learning_rate': 3.295903514244126e-06, 'epoch': 2.81}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4809/4809 [09:06<00:00,  8.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 546.4301, 'train_samples_per_second': 35.187, 'train_steps_per_second': 8.801, 'train_loss': 3.4617446769322493, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./body-fine-tuned-gpt2\\\\tokenizer_config.json',\n",
              " './body-fine-tuned-gpt2\\\\special_tokens_map.json',\n",
              " './body-fine-tuned-gpt2\\\\vocab.json',\n",
              " './body-fine-tuned-gpt2\\\\merges.txt',\n",
              " './body-fine-tuned-gpt2\\\\added_tokens.json')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    logging_steps=500,\n",
        "    logging_dir='C:/Users/pete/OneDrive/Desktop/404_finalproj/logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=body_train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained('./body-fine-tuned-gpt2')\n",
        "tokenizer.save_pretrained('./body-fine-tuned-gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JPcu28Uf9D9",
        "outputId": "44450610-9960-4f3d-9e1d-0215b4371596"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated post: Today in the stock market, there are a lot of people who are holding GME. I‚Äôm not one of them. But I do have a few shares of GMEs.\n",
            "\n",
            "I‚Äùm holding them because I believe in them and I want to see them go. They are the ones who will make us rich. We‚Äöre all going to have to pay for the GMES. \n",
            "  \n",
            "\n",
            "\n",
            "&#x200B;\n",
            "\n",
            "\n",
            "  I have been holding these shares for a while now. When I first started holding, I was holding $GME. Now I hold $AMC. And I am holding AMC. So I think it‚Äòs time to buy AMC shares. It\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "body_model_name = './body-fine-tuned-gpt2'\n",
        "body_tokenizer = GPT2Tokenizer.from_pretrained(body_model_name)\n",
        "body_model = GPT2LMHeadModel.from_pretrained(body_model_name)\n",
        "\n",
        "def body_generate_post(prompt, max_length=150):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    outputs = body_model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    body_generated_post = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return body_generated_post\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Today in the stock market\"\n",
        "body_generated_post = body_generate_post(prompt)\n",
        "print(f\"Generated post: {body_generated_post}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPrVGopwf9D9"
      },
      "source": [
        "### Building a random post based on the body trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moh9uvKHf9D9",
        "outputId": "57080115-569c-427e-fd58-cfef049129c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Token ID: 21767\n",
            "Random Token Text:  equals\n",
            "Generated post:  equals the amount of money you have to invest in a stock. \n",
            "\n",
            "If you are a billionaire and you own a company, you can buy a share of the company at a discount. If you hold a lot of shares, it is a good idea to buy shares at the discount, because the stock price will rise.\n",
            "  \n",
            "\n",
            "\n",
            "The stock market is rigged. The stock is manipulated. It is not a market where you buy and hold shares. You are buying shares for the price you want to pay. This is why the market has been rigged so that the hedge funds and other big players have been able to manipulate the markets to their advantage. They have manipulated the prices of all the other stocks to make the shares more expensive\n"
          ]
        }
      ],
      "source": [
        "# Flatten the list of tokenized titles\n",
        "body_flat_token_list = [token for sublist in body_train_dataset for token in sublist]\n",
        "\n",
        "# Select a random token\n",
        "body_random_token = random.choice(body_flat_token_list)\n",
        "\n",
        "# Optional: Convert the token back to text to see what it is\n",
        "body_random_token_text = body_tokenizer.decode([body_random_token])\n",
        "\n",
        "print(\"Random Token ID:\", body_random_token)\n",
        "print(\"Random Token Text:\", body_random_token_text)\n",
        "\n",
        "body_generated_random_post = body_generate_post(body_random_token_text)\n",
        "print(f\"Generated post: {body_generated_random_post}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXBk6YWMf9D-"
      },
      "outputs": [],
      "source": [
        "body_post_title = generate_title(body_generated_random_post, summarization_tokenizer, summarization_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ94M5ZBf9D-",
        "outputId": "6418dd52-8790-4a3e-8073-0c096b0ea518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Token ID: 21767\n",
            "Random Token Text:  equals\n",
            "Generated title: If you are a billionaire and you own a company, you can buy a share of the company at a discount. If you hold a lot of shares, it is a good idea to buy shares at the discount, because the stock price\n",
            "Generated post:  equals the amount of money you have to invest in a stock. \n",
            "\n",
            "If you are a billionaire and you own a company, you can buy a share of the company at a discount. If you hold a lot of shares, it is a good idea to buy shares at the discount, because the stock price will rise.\n",
            "  \n",
            "\n",
            "\n",
            "The stock market is rigged. The stock is manipulated. It is not a market where you buy and hold shares. You are buying shares for the price you want to pay. This is why the market has been rigged so that the hedge funds and other big players have been able to manipulate the markets to their advantage. They have manipulated the prices of all the other stocks to make the shares more expensive\n"
          ]
        }
      ],
      "source": [
        "print(\"Random Token ID:\", body_random_token)\n",
        "print(\"Random Token Text:\", body_random_token_text)\n",
        "print(f\"Generated title: {body_post_title}\")\n",
        "print(f\"Generated post: {body_generated_random_post}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}